{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "funny-matter",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy\n",
    "import string\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "patient-anchor",
   "metadata": {},
   "source": [
    "# Tokenising a string\n",
    "\n",
    "in order to process text we need to break it down into tokens. As we explained at the start, a token is a letter, word, number, or punctuation which is contained in a string. To tokenise we first need to import the word_tokenize method from the tokenize package from NLTK which allows us to do this without writing the code ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "chicken-confidentiality",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stylish-nutrition",
   "metadata": {},
   "source": [
    "We will also download a specific tokeniser that NLTK uses as default. There are different ways of tokenising text and today we will use NLTKâ€™s in-built punkt tokeniser by calling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "contrary-hamburg",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DIPTO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-sperm",
   "metadata": {},
   "source": [
    "Now we can assign text as a string variable and tokenise it. We will save the tokenised output in a list using the humpty_tokens variable. We can inspect this list by inspecting the humpty_tokens variable.\n",
    "\n",
    "### Using the word_tokenize to tokenize the string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dental-attempt",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Humpty', 'Dumpty', 'sat', 'on', 'a', 'wall', ',', 'Humpty', 'Dumpty', 'had']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humpty_string = \"Humpty Dumpty sat on a wall, Humpty Dumpty had a great fall; All the king's horses and all the king's men couldn't put Humpty together again.\"\n",
    "humpty_tokens = word_tokenize(humpty_string)\n",
    "# Show first 10 entries of the tokens list\n",
    "humpty_tokens[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "manual-oakland",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'great', 'fall', ';', 'All', 'the', 'king', \"'s\", 'horses', 'and']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humpty_tokens[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "valuable-character",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "humpty_tokens[15]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "laughing-fusion",
   "metadata": {},
   "source": [
    "As you can see, some of the words are uppercase and some are lowercase. To further analyse the data, for example counting the occurrences of a word, we need to normalise the data and make it all lowercase.\n",
    "\n",
    "You can lowercase the strings in the list by going through it and calling the .lower() method on each entry. You can do this by using a for loop to loop through each word in the list.\n",
    "\n",
    "### For lower case we use lower() method on each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "understood-transport",
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_case = [word.lower() for word in humpty_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "bound-greeting",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['humpty', 'dumpty', 'sat', 'on', 'a', 'wall', ',', 'humpty', 'dumpty', 'had']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_case[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cognitive-algeria",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'great', 'fall', ';', 'all', 'the', 'king', \"'s\", 'horses', 'and']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_case[10:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "molecular-atlantic",
   "metadata": {},
   "source": [
    "### For upper case we use upper() method on each entry."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "welsh-groove",
   "metadata": {},
   "outputs": [],
   "source": [
    "upper_case = [word.upper() for word in humpty_tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "promotional-desire",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['HUMPTY', 'DUMPTY', 'SAT', 'ON', 'A', 'WALL', ',', 'HUMPTY', 'DUMPTY', 'HAD']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_case[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "skilled-chinese",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A', 'GREAT', 'FALL', ';', 'ALL', 'THE', 'KING', \"'S\", 'HORSES', 'AND']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_case[10:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "constant-somewhere",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'FALL'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "upper_case[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "continuous-footwear",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'fall'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lower_case[12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-sympathy",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
